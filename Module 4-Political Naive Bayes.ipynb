{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Muris Saab\n",
    "ADS509 - Assignment 4.1\n",
    "University of San Diego\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/muriss/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/muriss/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/muriss/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/muriss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/muriss/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    text = text.lower() \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "    '''\n",
    "    SELECT text, party\n",
    "    FROM conventions                            \n",
    "    '''\n",
    ")\n",
    "\n",
    "for row in query_results :\n",
    "    text = row[0]  # the raw text\n",
    "    party = row[1]  # the associated party\n",
    "    cleaned_text = clean_tokenize(text)\n",
    "    convention_data.append([cleaned_text, party])\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['hereby',\n",
       "   'endorsing',\n",
       "   'joe',\n",
       "   'biden',\n",
       "   'next',\n",
       "   'president',\n",
       "   'united',\n",
       "   'states'],\n",
       "  'Democratic'],\n",
       " [['us',\n",
       "   'knowing',\n",
       "   'power',\n",
       "   'kamala',\n",
       "   'harris',\n",
       "   'america',\n",
       "   'us',\n",
       "   'knowing',\n",
       "   'power',\n",
       "   'us',\n",
       "   'live',\n",
       "   'people',\n",
       "   'right',\n",
       "   'remind',\n",
       "   'see',\n",
       "   'hear',\n",
       "   'matter',\n",
       "   'going',\n",
       "   'vice',\n",
       "   'president',\n",
       "   'united',\n",
       "   'states'],\n",
       "  'Democratic'],\n",
       " [['together',\n",
       "   'nations',\n",
       "   'europe',\n",
       "   'experienced',\n",
       "   'greater',\n",
       "   'increase',\n",
       "   'excess',\n",
       "   'mortality',\n",
       "   'united',\n",
       "   'states',\n",
       "   'think',\n",
       "   'enacted',\n",
       "   'largest',\n",
       "   'package',\n",
       "   'financial',\n",
       "   'relief',\n",
       "   'american',\n",
       "   'history',\n",
       "   'thanks',\n",
       "   'paycheck',\n",
       "   'protection',\n",
       "   'program',\n",
       "   'saved',\n",
       "   'supported',\n",
       "   'million',\n",
       "   'american',\n",
       "   'jobs',\n",
       "   'thats',\n",
       "   'one',\n",
       "   'reasons',\n",
       "   'advancing',\n",
       "   'rapidly',\n",
       "   'economy',\n",
       "   'great',\n",
       "   'job',\n",
       "   'result',\n",
       "   'seen',\n",
       "   'smallest',\n",
       "   'economic',\n",
       "   'contraction',\n",
       "   'major',\n",
       "   'western',\n",
       "   'nation',\n",
       "   'recovering',\n",
       "   'much',\n",
       "   'faster',\n",
       "   'rate',\n",
       "   'anybody',\n",
       "   'past',\n",
       "   'three',\n",
       "   'months',\n",
       "   'gained',\n",
       "   'nine',\n",
       "   'million',\n",
       "   'jobs',\n",
       "   'thats',\n",
       "   'record',\n",
       "   'history',\n",
       "   'country',\n",
       "   'unfortunately',\n",
       "   'beginning',\n",
       "   'opponents',\n",
       "   'shown',\n",
       "   'capable',\n",
       "   'nothing',\n",
       "   'partisan',\n",
       "   'ability',\n",
       "   'criticize',\n",
       "   'took',\n",
       "   'bold',\n",
       "   'action',\n",
       "   'issue',\n",
       "   'travel',\n",
       "   'ban',\n",
       "   'china',\n",
       "   'early',\n",
       "   'indeed',\n",
       "   'joe',\n",
       "   'biden',\n",
       "   'called',\n",
       "   'hysterical',\n",
       "   'xenophobic',\n",
       "   'introduced',\n",
       "   'ban',\n",
       "   'europe',\n",
       "   'early',\n",
       "   'listened',\n",
       "   'joe',\n",
       "   'hundreds',\n",
       "   'thousands',\n",
       "   'americans',\n",
       "   'would',\n",
       "   'died',\n",
       "   'instead',\n",
       "   'following',\n",
       "   'science',\n",
       "   'joe',\n",
       "   'biden',\n",
       "   'wants',\n",
       "   'inflict',\n",
       "   'painful',\n",
       "   'shutdown',\n",
       "   'entire',\n",
       "   'country',\n",
       "   'shutdown',\n",
       "   'would',\n",
       "   'inflict',\n",
       "   'unthinkable',\n",
       "   'lasting',\n",
       "   'harm',\n",
       "   'nations',\n",
       "   'childrens',\n",
       "   'families',\n",
       "   'citizens',\n",
       "   'backgrounds'],\n",
       "  'Republican'],\n",
       " [['letters',\n",
       "   'harder',\n",
       "   'others',\n",
       "   'used',\n",
       "   'get',\n",
       "   'night',\n",
       "   'go',\n",
       "   'stand',\n",
       "   'front',\n",
       "   'mirror',\n",
       "   'flashlight',\n",
       "   'practice',\n",
       "   'shed',\n",
       "   'make',\n",
       "   'look',\n",
       "   'eye',\n",
       "   'look',\n",
       "   'remember',\n",
       "   'joey',\n",
       "   'youre',\n",
       "   'smartest',\n",
       "   'boy',\n",
       "   'class',\n",
       "   'nobodys',\n",
       "   'better',\n",
       "   'joey',\n",
       "   'deal',\n",
       "   'stuttering',\n",
       "   'gave',\n",
       "   'insight',\n",
       "   'peoples',\n",
       "   'pain',\n",
       "   'peoples',\n",
       "   'suffering'],\n",
       "  'Democratic'],\n",
       " [['kids',\n",
       "   'going',\n",
       "   'look',\n",
       "   'say',\n",
       "   'best',\n",
       "   'mom',\n",
       "   'working',\n",
       "   'raising',\n",
       "   'us',\n",
       "   'tremendous',\n",
       "   'could'],\n",
       "  'Republican'],\n",
       " [['hi',\n",
       "   'im',\n",
       "   'congressman',\n",
       "   'dan',\n",
       "   'crenshaw',\n",
       "   'eight',\n",
       "   'years',\n",
       "   'ago',\n",
       "   'fields',\n",
       "   'helmand',\n",
       "   'province',\n",
       "   'afghanistan',\n",
       "   'close',\n",
       "   'friend',\n",
       "   'teammate',\n",
       "   'laid',\n",
       "   'cover',\n",
       "   'fire',\n",
       "   'taliban',\n",
       "   'surgeons',\n",
       "   'could',\n",
       "   'walk',\n",
       "   'blind',\n",
       "   'bloodied',\n",
       "   'medevac',\n",
       "   'helicopter',\n",
       "   'survive',\n",
       "   'didnt',\n",
       "   'dave',\n",
       "   'orson',\n",
       "   'killed',\n",
       "   'two',\n",
       "   'months',\n",
       "   'later',\n",
       "   'died',\n",
       "   'hero',\n",
       "   'great',\n",
       "   'country',\n",
       "   'heres',\n",
       "   'truth',\n",
       "   'america',\n",
       "   'country',\n",
       "   'heroes',\n",
       "   'believe',\n",
       "   'people',\n",
       "   'common',\n",
       "   'set',\n",
       "   'ideals',\n",
       "   'conceived',\n",
       "   'liberty',\n",
       "   'people',\n",
       "   'sacrificed',\n",
       "   'time',\n",
       "   'freedom',\n",
       "   'freedom',\n",
       "   'others',\n",
       "   'thats',\n",
       "   'something',\n",
       "   'country',\n",
       "   'ever',\n",
       "   'anywhere',\n",
       "   'claim',\n",
       "   'since',\n",
       "   'ive',\n",
       "   'seen',\n",
       "   'americas',\n",
       "   'heroes',\n",
       "   'close',\n",
       "   'saved',\n",
       "   'life',\n",
       "   'saved',\n",
       "   'many',\n",
       "   'others',\n",
       "   'lives',\n",
       "   'many',\n",
       "   'never',\n",
       "   'made',\n",
       "   'home',\n",
       "   'heroes',\n",
       "   'acted',\n",
       "   'whole',\n",
       "   'struggle',\n",
       "   'depended',\n",
       "   'alone',\n",
       "   'weakness',\n",
       "   'part',\n",
       "   'would',\n",
       "   'reflection',\n",
       "   'whole',\n",
       "   'nation'],\n",
       "  'Republican'],\n",
       " [['lets',\n",
       "   'join',\n",
       "   'together',\n",
       "   'committing',\n",
       "   'help',\n",
       "   'children',\n",
       "   'dream',\n",
       "   'big',\n",
       "   'think',\n",
       "   'big',\n",
       "   'best',\n",
       "   'everything'],\n",
       "  'Republican'],\n",
       " [['mean', 'socialist'], 'Republican'],\n",
       " [['thats',\n",
       "   'joes',\n",
       "   'plan',\n",
       "   'create',\n",
       "   'millions',\n",
       "   'new',\n",
       "   'good',\n",
       "   'paying',\n",
       "   'jobs',\n",
       "   'many',\n",
       "   'union',\n",
       "   'like',\n",
       "   'mine',\n",
       "   'invest',\n",
       "   'critical',\n",
       "   'infrastructure',\n",
       "   'upgrade',\n",
       "   'millions',\n",
       "   'buildings',\n",
       "   'invest',\n",
       "   'micro',\n",
       "   'mobility',\n",
       "   'precision',\n",
       "   'agriculture',\n",
       "   'clean',\n",
       "   'energy',\n",
       "   'future',\n",
       "   'achieves',\n",
       "   'net',\n",
       "   'zero',\n",
       "   'emissions',\n",
       "   'cant',\n",
       "   'power',\n",
       "   'economies',\n",
       "   'future',\n",
       "   'without',\n",
       "   'investing',\n",
       "   'technologies',\n",
       "   'future'],\n",
       "  'Democratic'],\n",
       " [['democrats',\n",
       "   'republicans',\n",
       "   'want',\n",
       "   'thank',\n",
       "   'much',\n",
       "   'thank',\n",
       "   'everybody',\n",
       "   'whole',\n",
       "   'beautiful',\n",
       "   'post',\n",
       "   'office',\n",
       "   'system',\n",
       "   'appreciate'],\n",
       "  'Republican']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2327 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t] # .split()] # Since the clean_tokenize function has already tokenized the text, there's no need to split the tokens again.\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text, fw):\n",
    "    \"\"\"\n",
    "    Given some text, this returns a dictionary holding the feature words.\n",
    "\n",
    "    Args: \n",
    "        * text: a piece of text or tokenized text.\n",
    "        * fw: the *feature words* that we're considering. A word \n",
    "        in `text` must be in fw in order to be returned.\n",
    "    \n",
    "    Returns: \n",
    "        A dictionary with the words in `text` that appear in `fw`. \n",
    "        Words are only counted once. \n",
    "    \"\"\"\n",
    "    \n",
    "    # If text is a string, tokenize it\n",
    "    if isinstance(text, str):\n",
    "        words = nltk.word_tokenize(text)\n",
    "    else:\n",
    "        words = text  # If text is already tokenized\n",
    "    \n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Initialize the return dictionary\n",
    "    ret_dict = dict()\n",
    "    \n",
    "    # Iterate over the words in the text and check if they are in the feature words\n",
    "    for word in words:\n",
    "        lemma_word = lemmatizer.lemmatize(word)\n",
    "        if lemma_word in fw:\n",
    "            ret_dict[lemma_word] = True\n",
    "    \n",
    "    return ret_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.474\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     27.1 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
      "                religion = True           Republ : Democr =     16.1 : 1.0\n",
      "                 liberal = True           Republ : Democr =     14.0 : 1.0\n",
      "                 defense = True           Republ : Democr =     13.0 : 1.0\n",
      "                  defund = True           Republ : Democr =     13.0 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.8 : 1.0\n",
      "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
      "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
      "               terrorist = True           Republ : Democr =     11.9 : 1.0\n",
      "               amendment = True           Republ : Democr =     10.9 : 1.0\n",
      "                 culture = True           Republ : Democr =     10.9 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
      "                   crime = True           Republ : Democr =     10.3 : 1.0\n",
      "                    mike = True           Republ : Democr =     10.3 : 1.0\n",
      "                preserve = True           Republ : Democr =     10.3 : 1.0\n",
      "                  record = True           Republ : Democr =     10.0 : 1.0\n",
      "                   armed = True           Republ : Democr =      9.9 : 1.0\n",
      "                    iran = True           Republ : Democr =      9.9 : 1.0\n",
      "                  relief = True           Republ : Democr =      9.9 : 1.0\n",
      "                   third = True           Republ : Democr =      9.9 : 1.0\n",
      "               wonderful = True           Republ : Democr =      9.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "- **Republican Keywords**:\n",
    "   Many of the most informative features appear to be strongly associated with the Republican party. Words like \"china,\" \"enforcement,\" \"destroy,\" \"liberal,\" \"defund,\" \"flag,\" and \"trade\" appear frequently in Republican speeches, as evidenced by the high ratios (e.g., \"china\" is 27.1 times more likely to appear in Republican speeches than Democratic ones). This suggests that the Republican convention focused heavily on issues such as foreign policy (e.g., \"china\"), law and order (\"enforcement,\" \"crime\"), national pride (\"flag\"), and cultural criticism (\"liberal,\" \"defund\").\n",
    "\n",
    "- **Democratic Keywords**:\n",
    "   On the Democratic side, fewer features stand out, but \"climate\" is a prominent word associated with Democratic speeches. This aligns with the Democratic party’s focus on addressing climate change, which is often a major platform issue for them.\n",
    "\n",
    "- **Polarity**:\n",
    "   There is a stark polarity in the words used by the two parties, especially in the context of security and national defense. For example, words like \"enforcement,\" \"defund,\" \"defense,\" and \"terrorist\" are highly associated with Republican speeches. These reflect themes around law enforcement, military strength, and national security, which are traditionally more emphasized by Republicans.\n",
    "\n",
    "- **Unusual Words**:\n",
    "   Some words like \"mike\" and \"abraham\" appear as highly informative features for Republicans. These could refer to prominent figures (e.g., Mike Pence or Abraham Lincoln), which may indicate that these figures were commonly referenced during Republican speeches.\n",
    "\n",
    "- **Potential Bias**:\n",
    "   It’s also interesting that the classifier seems more capable of identifying features associated with Republican speeches than Democratic ones, given the higher number of informative features for the Republican party. This could suggest that Republican speeches in this dataset are more distinct in their word choice compared to Democratic speeches. Alternatively, it might reflect imbalances in the dataset or differences in how the two parties frame their key issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mo Brooks',\n",
       "  'Republican',\n",
       "  b'\"Brooks Joins Alabama Delegation in Voting Against Flawed Funding Bill\" http://t.co/3CwjIWYsNq')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming\n",
    "\n",
    "results[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['brooks', 'joins', 'alabama', 'delegation', 'voting', 'flawed', 'funding', 'bill'], 'Republican'], [['brooks', 'senate', 'democrats', 'allowing', 'president', 'give', 'americans', 'jobs', 'illegals', 'securetheborder'], 'Republican'], [['nasa', 'square', 'event', 'sat', 'pm', 'stop', 'amp', 'hear', 'incredible', 'work', 'done', 'al', 'downtownhsv'], 'Republican'], [['trouble', 'socialism', 'eventually', 'run', 'peoples', 'money', 'margaret', 'thatcher'], 'Republican'], [['trouble', 'socialism', 'eventually', 'run', 'peoples', 'money', 'thatcher', 'shell', 'sorely', 'missed'], 'Republican']]\n"
     ]
    }
   ],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "\n",
    "for row in results:\n",
    "    name = row[0] \n",
    "    party = row[1]  \n",
    "    tweet = row[2].decode('utf-8') \n",
    "    cleaned_text = clean_tokenize(tweet)\n",
    "    tweet_data.append([cleaned_text, party])\n",
    "\n",
    "print(tweet_data[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: ['liz', 'indiana', 'like', 'line', 'dance', 'amp', 'good', 'time']\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: ['every', 'day', 'sit', 'wait', 'people', 'infected', 'sayfie']\n",
      "Actual party is Republican and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: ['realdonaldtrump', 'promised', 'hed', 'end', 'radical', 'obama', 'jobkilling', 'regulations', 'coal', 'restore', 'american', 'energy', 'independence', 'hes', 'delivering', 'todays', 'action', 'means', 'jobs', 'lower', 'prices', 'return', 'american', 'strength']\n",
      "Actual party is Republican and our classifier says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: ['thank', 'yous', 'around', 'reelection', 'time', 'heads']\n",
      "Actual party is Republican and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: ['speed', 'walk', 'run', 'click', 'link', 'register', 'vote', 'deadline', 'tomorrow', 'register', 'make', 'sure', 'voice', 'heard', 'november']\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: ['great', 'time', 'nebraskadems', 'today', 'happy', 'bring', 'two', 'first', 'time', 'attendees']\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: ['protecting', 'funding', 'finite', 'natural', 'resources', 'permanently', 'ny']\n",
      "Actual party is Republican and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: ['democrats', 'take', 'back', 'house', 'fight', 'universal', 'healthcare', 'gun', 'control', 'workers', 'rights', 'equality', 'minorities', 'lgbtq', 'folks', 'women', 'amp', 'seniors', 'cant', 'alone', 'chip', 'deadline', 'amp', 'help', 'flip', 'ny']\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: ['thank']\n",
      "Actual party is Republican and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: ['posted', 'photos', 'facebook', 'album', 'chiefland', 'watermelon', 'parade', 'festival']\n",
      "Actual party is Republican and our classifier says Republican.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tweet_features(words):\n",
    "    return {word: True for word in words}\n",
    "featuresets = [(tweet_features(tweet), party) for tweet, party in tweet_data]\n",
    "\n",
    "random.seed(20201014)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "train_set = featuresets[:int(len(featuresets) * 0.8)]\n",
    "test_set = featuresets[int(len(featuresets) * 0.8):]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "for tweet, party in tweet_data_sample:\n",
    "    features = tweet_features(tweet)\n",
    "    estimated_party = classifier.classify(features)\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifier says {estimated_party}.\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican', 'Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties:\n",
    "    for p1 in parties:\n",
    "        results[p][p1] = 0\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data):\n",
    "    tweet, party = tp  \n",
    "    features = tweet_features(tweet)\n",
    "    estimated_party = classifier.classify(features)\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx >= num_to_score:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 3073, 'Democratic': 1277}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 389, 'Democratic': 5262})})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "The results can be interpreted as a **confusion matrix**, showing how well the classifier is performing in terms of classifying tweets as either **Republican** or **Democratic**.\n",
    "\n",
    "- `'Republican': {'Republican': 3073, 'Democratic': 1277}`:\n",
    "  - **3073** tweets from actual Republicans were correctly classified as **Republican**.\n",
    "  - **1277** tweets from actual Republicans were incorrectly classified as **Democratic**.\n",
    "\n",
    "- `'Democratic': {'Republican': 389, 'Democratic': 5262}`:\n",
    "  - **5262** tweets from actual Democrats were correctly classified as **Democratic**.\n",
    "  - **389** tweets from actual Democrats were incorrectly classified as **Republican**.\n",
    "\n",
    "### Observations:\n",
    "1. **Accuracy**:\n",
    "   - The classifier performs well, with the majority of tweets being classified correctly for both parties:\n",
    "     - **Republican tweets**: \\( \\frac{3073}{3073 + 1277} \\approx 70.6\\% \\) accuracy for Republican tweets.\n",
    "     - **Democratic tweets**: \\( \\frac{5262}{5262 + 389} \\approx 93.1\\% \\) accuracy for Democratic tweets.\n",
    "\n",
    "2. **Class Imbalance**:\n",
    "   - The classifier is more accurate at predicting **Democratic** tweets than **Republican** tweets. This could be due to a number of factors:\n",
    "     - **More distinctive language** in Democratic tweets.\n",
    "     - **Possible class imbalance**: If there are more Democratic tweets in the dataset, the classifier might be biased toward predicting the Democratic party more often.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
